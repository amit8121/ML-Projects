{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINAL_CNN_ALL_Images",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRct6G_O-Zop"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mb_im\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "\n",
        "import keras\n",
        "keras.backend.set_floatx('float64')\n",
        "\n",
        "from keras.activations import relu\n",
        "from keras.losses import CategoricalCrossentropy\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "CCE = CategoricalCrossentropy()\n",
        "\n",
        "from scipy.special import softmax\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVk61ltrl4Lu"
      },
      "source": [
        "digits = load_digits()\n",
        "\n",
        "image_dataset = digits.images\n",
        "image_label = digits.target\n",
        "\n",
        "random_img_indices = random.sample(range(0, len(image_dataset))  , k = 1797  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DqKWaYUrD3F"
      },
      "source": [
        "\n",
        "ori_h, ori_w = image_dataset[0].shape\n",
        "ori_c = 1\n",
        "\n",
        "lb.fit(image_label[:10])\n",
        "\n",
        "\n",
        "X = image_dataset[random_img_indices[0]].reshape(1,ori_h, ori_w, ori_c)\n",
        "Y = lb.transform( [image_label[random_img_indices[0]]     ] ).reshape(1,-1)\n",
        "\n",
        "\n",
        "\n",
        "for ri in random_img_indices[1: ]:\n",
        "\n",
        "  X = np.vstack((X, image_dataset[  random_img_indices[ri]                 ].reshape(1,ori_h, ori_w, ori_c)   ))\n",
        "\n",
        "  Y = np.vstack((Y, lb.transform( [ image_label[random_img_indices[ri]]    ] ).reshape(1,-1) ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7zczB8Gslyx",
        "outputId": "ef4dbded-2a32-4b9a-f65d-3801f7f2fcd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X.shape, Y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797, 8, 8, 1), (1797, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWcS3TmBTyI4"
      },
      "source": [
        "\n",
        "def yield_single_image(X_dataset):\n",
        "\n",
        "  for each_ind, each_image in enumerate(X_dataset):\n",
        "    yield (each_ind, each_image)\n",
        "\n",
        "def yield_image_patch(inp_im , im_h, im_w, fh, fw, im_c = 1,\n",
        "                      sh=1, sw=1, pad_h = 1, pad_w = 1,\n",
        "                      op = \"convolution\" ):\n",
        "  \n",
        "  \"\"\" This function yields a patch of single image, based on filter dimensions, to perform convolution operation  \"\"\"\n",
        "\n",
        "  \n",
        "  \n",
        "    \n",
        "  it_h = int(1 + (im_h - fh) / sh)\n",
        "\n",
        "  it_w = int(1 + (im_w - fw) / sw)\n",
        "\n",
        "  \n",
        "  for h in range(it_h):\n",
        "    for w in range(it_w):\n",
        "      \n",
        "      yield (h,w, inp_im[h*sh: (h*sh) + fh,\n",
        "                        w*sw : (w*sw) + fw] )\n",
        "\n",
        "def relu_derivative(A):\n",
        "  A[A<=0] = 0\n",
        "  A[A>0] = 1\n",
        "  return A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul3jLeyzBZQ2"
      },
      "source": [
        "class My_Conv2D:\n",
        "\n",
        "  def __init__(self, num_feature_maps = 7, filter_size = 7, stride = (1,1),\n",
        "               padding = 'VALID', padding_size = None, input_shape = None ):\n",
        "    \n",
        "\n",
        "    \"\"\" \n",
        "    This class performns convolution operation, without any activation applied on it, on an entire data set,\n",
        "     this class provides both forward and back ward propogation implementations.\n",
        "     \n",
        "     This class only supports square filter and stride shapes for time being, also the input shape should be 4D in following format,\n",
        "     (num_samples, input_height, input_width, input_channels).\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "    self.num_feature_maps = num_feature_maps\n",
        "\n",
        "    self.fh, self.fw = filter_size, filter_size\n",
        "\n",
        "    self.stride_h, self.stride_w = stride\n",
        "\n",
        "\n",
        "    # If padding is 'valid' then it means no padding\n",
        "    if padding == 'VALID':\n",
        "\n",
        "      # Setting padding height and width\n",
        "      self.pad_h, self.pad_w = 0,0\n",
        "\n",
        "    \n",
        "    elif padding == 'SAME':\n",
        "\n",
        "      # Setting padding height and width\n",
        "      self.pad_h , self.pad_w = padding_size, padding_size\n",
        "\n",
        "\n",
        "    # If input shape is already provided\n",
        "    if input_shape:\n",
        "\n",
        "      #Assumed to be 4D\n",
        "      self.input_shape = input_shape\n",
        "\n",
        "      self._initialize_placeholders()\n",
        "      \n",
        "\n",
        "    # If input shape is not provided yet, we don't have enough info to create filter weights\n",
        "    else:\n",
        "      pass\n",
        "  \n",
        "  def _initialize_placeholders(self, mode = \"forward\"):\n",
        "\n",
        "    \"\"\" Function to initialize required filter and convolution output placeholder variables, this function can be used to initialize both\n",
        "    forward and backward propogation placeholders.  \"\"\"\n",
        "\n",
        "    if mode == \"forward\":\n",
        "\n",
        "      # Getting required variables from input shape tuple\n",
        "      self.num_samples, self.ih, self.iw, self.ic = self.input_shape\n",
        "\n",
        "      # Calculating output shape after convolution operation\n",
        "      o_height = int(  (self.ih - self.fh + 2*self.pad_h ) / self.stride_h   ) + 1\n",
        "      o_width  = int(  (self.iw - self.fw + 2*self.pad_w ) / self.stride_w   ) + 1\n",
        "\n",
        "      # Setting convolution height, width and no. of feature maps\n",
        "      self.co_h , self.co_w, self.co_fm = o_height, o_width, self.num_feature_maps\n",
        "\n",
        "\n",
        "      # A filter of a convolution layer is always 4D - (filter height, filter width, no.of input channels, no.of output feature maps)\n",
        "      # Defining based on hyper parameters and normalizing filter values\n",
        "      self.filter = np.random.randn(self.fh, self.fw, self.ic , self.num_feature_maps) / (self.fh * self.fw)\n",
        "\n",
        "\n",
        "      # A convolution output for a single image is always 4D if num of samples are given\n",
        "      self.conv_out = np.zeros(shape = (self.num_samples, self.co_h, self.co_w, self.co_fm) , dtype = np.float32)\n",
        "\n",
        "\n",
        "    else:\n",
        "\n",
        "      \n",
        "      filter_placeholder = np.zeros(shape = (self.fh, self.fw, self.ic, self.num_feature_maps ))\n",
        "\n",
        "      incoming_image_placeholder = np.zeros(shape = (self.num_samples, self.ih, self.iw, self.ic))\n",
        "      \n",
        "      # Returning placeholders for holding filter and input derivatives w.r.t this layer\n",
        "      return (filter_placeholder,\n",
        "              incoming_image_placeholder)\n",
        "\n",
        "  def _yield_image_patch(self, input_image):\n",
        "\n",
        "    \n",
        "    # n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
        "    # n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
        "\n",
        "\n",
        "    it_h = int( (self.ih - self.fh + (2*self.pad_h) ) / self.stride_h ) + 1\n",
        "\n",
        "    it_w = int( (self.iw - self.fw + (2*self.pad_w) ) / self.stride_w ) + 1\n",
        "\n",
        "      \n",
        "    for h in range(it_h):\n",
        "      for w in range(it_w):\n",
        "\n",
        "        height_start = h * self.stride_h; height_end = height_start + self.fh\n",
        "        width_start = w * self.stride_w ; width_end = width_start + self.fw\n",
        "\n",
        "        # Yielding single image, of the shape of filter dimensions and all channels \n",
        "        yield (h,w, input_image[height_start: height_end,\n",
        "                          width_start:width_end, : ] )\n",
        "\n",
        "  def forward_prop(self, dataset ):\n",
        "\n",
        "    # Filter has already been defined, continue with normal convolution operation\n",
        "    if 'filter' in self.__dict__ and 'conv_out' in self.__dict__:\n",
        "      pass\n",
        "\n",
        "\n",
        "    # We still don't know the no. of channels for image, so we have to initialize the filter first with proper channels before training\n",
        "    else:\n",
        "      \n",
        "      # dataset has 4D here ( m, h, w, c)\n",
        "      if len(dataset.shape) > 3:\n",
        "        self.input_shape = dataset.shape\n",
        "        \n",
        "      else:\n",
        "        num_samp, im_h, im_w = input_image.shape; im_c = 1\n",
        "        dataset = dataset.reshape(num_samp, imh_h, im_c, im_c)\n",
        "\n",
        "        self.input_shape = (num_samp, im_h, im_w, im_c)\n",
        "      \n",
        "      # Initialize filter weights and other placeholder variables\n",
        "      self._initialize_placeholders()\n",
        "\n",
        "    \n",
        "    self.dataset = dataset\n",
        "\n",
        "    for img_ind , input_image in yield_single_image(self.dataset):\n",
        "\n",
        "      for h, w, image_patch in self._yield_image_patch(input_image = input_image ):\n",
        "\n",
        "        for feat_map in range(self.num_feature_maps):\n",
        "  \n",
        "          self.conv_out[ img_ind , h, w, feat_map] = np.sum(   image_patch * self.filter[: , : , : , feat_map]  )\n",
        "          \n",
        "  def backward_prop(self, DL_nextL, learning_rate):\n",
        "\n",
        "    \"\"\" This function computes back-propogation values w.r.t the filter of this particular convolution layer. \n",
        "\n",
        "    This function assumes that back propogation for layers after this were already computed.\n",
        "     \"\"\"\n",
        "    \n",
        "    # Returns a numpy array of zeros with proper shape for filter \n",
        "    self.DL_filter, self.DL_input = self._initialize_placeholders(mode = \"backward\")\n",
        "\n",
        "\n",
        "    # For each image in entire dataset\n",
        "    for each_image_ind, each_image in yield_single_image(self.dataset):\n",
        "      \n",
        "      for h, w, each_im_patch in self._yield_image_patch(input_image = each_image ):\n",
        "        \n",
        "        for each_feat_map in range(self.num_feature_maps):\n",
        "         \n",
        "          \n",
        "          self.DL_filter[:, : , :, each_feat_map] += ( each_im_patch * DL_nextL[each_image_ind, h, w, each_feat_map] )\n",
        "\n",
        "          self.DL_input[each_image_ind,\n",
        "                        h*self.stride_h : h*self.stride_h + self.fh,\n",
        "                        w*self.stride_w : w*self.stride_w + self.fw ,\n",
        "                        :  ] += ( DL_nextL[each_image_ind,h,w,each_feat_map] * self.filter[: , : , : , each_feat_map ])\n",
        "\n",
        "\n",
        "    self.DL_filter /= self.num_samples\n",
        "    \n",
        "    # Update the weights using an optimizer\n",
        "    self.filter -= (learning_rate * self.DL_filter)\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMyxdpw51Bw6"
      },
      "source": [
        "class My_Pooling:\n",
        "\n",
        "  def __init__(self, pool_size = 2, stride = 1, pool_type = \"Max\", padding = 'valid'):\n",
        "\n",
        "    self.ph, self.pw = pool_size, pool_size\n",
        "\n",
        "    self.stride_h, self.stride_w = stride\n",
        "\n",
        "  def _initialize_placeholders(self, mode = \"forward\"):\n",
        "\n",
        "    \"\"\" Function to initialize required filter and convolution output placeholder variables  \"\"\"\n",
        "\n",
        "\n",
        "    if mode == \"forward\":\n",
        "\n",
        "      self.num_samples, self.ih, self.iw, self.ic = self.input_shape\n",
        "\n",
        "      # n_H = int(1 + (n_H_prev - f) / stride)\n",
        "      # n_W = int(1 + (n_W_prev - f) / stride)\n",
        "      # n_C = n_C_prev\n",
        "\n",
        "      o_H = int(  (self.ih - self.ph  ) / self.stride_h   ) + 1\n",
        "\n",
        "      o_W = int(  (self.iw - self.pw  ) / self.stride_w   ) + 1\n",
        "\n",
        "      o_C = self.ic\n",
        "\n",
        "      self.pool_out_h , self.pool_out_w, self.pool_out_c = o_H, o_W, o_C\n",
        "\n",
        "      # A convolution output for a single image is always 4D if num of samples are given\n",
        "      self.pool_out = np.zeros(shape = (self.num_samples, self.pool_out_h , self.pool_out_w, self.pool_out_c) , dtype = np.float32)\n",
        "\n",
        "\n",
        "    else:\n",
        "      return np.zeros(shape = (self.num_samples, self.ih, self.iw, self.ic ))\n",
        "\n",
        "\n",
        "  def _yield_image_patch(self, input_image):\n",
        "\n",
        "    # This might also has to change, but not necessarily\n",
        "    it_H = int(  (self.ih - self.ph  ) / self.stride_h   ) + 1\n",
        "\n",
        "    it_W = int(  (self.iw - self.pw  ) / self.stride_w   ) + 1;\n",
        "    \n",
        "    it_C = self.ic\n",
        "\n",
        "      \n",
        "    for h in range(it_H):\n",
        "      for w in range(it_W):\n",
        "        for c in range(it_C):\n",
        "\n",
        "          # Test with this height start, width start calc, if perf don't improve\n",
        "          # Change to vert_start = h; vert_end = h+f; horiz_start = w; horiz_end = w+f\n",
        "\n",
        "          height_start = h * self.stride_h; height_end = height_start + self.ph\n",
        "          width_start = w * self.stride_w ; width_end = width_start + self.pw\n",
        "\n",
        "          # New Approach\n",
        "          # height_start = h ; height_end = h + self.ph\n",
        "          # width_start = w ; width_end = w + self.pw\n",
        "\n",
        "\n",
        "\n",
        "        # Yielding single image, of the shape of filter dimensions and all channels \n",
        "          yield (h,w,c, input_image[height_start: height_end,\n",
        "                          width_start:width_end, c ] )\n",
        "\n",
        "\n",
        "  def max_forward(self):\n",
        "    pass\n",
        "\n",
        "  def max_backward(self):\n",
        "    pass\n",
        "\n",
        "  def average_forward(self, dataset):\n",
        "\n",
        "    # Input data to this layer\n",
        "    self.dataset = dataset\n",
        "\n",
        "    # Assuming this to be already in 4D, and first dimension for no. of samples\n",
        "    self.input_shape = self.dataset.shape\n",
        "\n",
        "    # Intializes required placeholders as class level variables\n",
        "    self._initialize_placeholders()\n",
        "\n",
        "  \n",
        "    for img_ind , input_image in yield_single_image(self.dataset):\n",
        "\n",
        "      for h, w, feat_map, image_patch in self._yield_image_patch(input_image = input_image):\n",
        "\n",
        "        self.pool_out[ img_ind , h, w, feat_map] = np.mean( image_patch )\n",
        "\n",
        "\n",
        "  def average_backward(self, DL_nextL):\n",
        "\n",
        "    # Intializes required placeholders as class level variables\n",
        "    self.DL_prev_Layer = self._initialize_placeholders(mode = \"backward\") # (m, dataset_height, dataset_width, dataset_feat_map)\n",
        "\n",
        "\n",
        "    for img_ind , input_image in yield_single_image(DL_nextL):\n",
        "\n",
        "      for h, w,feat_map, image_patch in self._yield_image_patch(input_image = input_image):\n",
        "        \n",
        "        distribute_val = DL_nextL[img_ind, h,w,feat_map] / (self.ph*self.pw);\n",
        "\n",
        "        a = np.ones(shape = (self.ph, self.pw ) ) * distribute_val  \n",
        "\n",
        "        self.DL_prev_Layer[ img_ind , (h*self.stride_h): (h*self.stride_h) + self.ph,\n",
        "                                      (w*self.stride_w) : (w*self.stride_w) + self.pw ,\n",
        "                            feat_map] +=  a\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "    # No Updates required for this step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ5mnfMU-wHu"
      },
      "source": [
        "# run_epochs = int(input(\"Please enter no. of epochs : \"))\n",
        "\n",
        "# learning_rate = float(input(\"Please enter no. of epochs : \"))\n",
        "\n",
        "\n",
        "run_epochs = 400\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "\n",
        "img_feat = 27; num_outs = 10; num_samples = X.shape[0]\n",
        "\n",
        "Weights = np.random.rand(num_outs, img_feat ) * np.sqrt(1/(num_outs + img_feat))\n",
        "\n",
        "Conv_Layer = My_Conv2D(num_feature_maps=3, filter_size=5, stride = (1,1), input_shape=(1797,8,8, 1))\n",
        "\n",
        "Pool_Layer = My_Pooling(pool_size = 2, stride = (1,1), pool_type = \"Avergae\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0EJ49KJkxs",
        "outputId": "1c2a9c49-f1fe-4791-b19c-a1bdc1e68710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "My_Losses = []\n",
        "\n",
        "\n",
        "for i in range(run_epochs):\n",
        "\n",
        "  Conv_Layer.forward_prop(dataset= X )\n",
        "  Z1 = Conv_Layer.conv_out\n",
        "\n",
        "  A1 = relu(Z1).numpy()\n",
        "\n",
        "  Pool_Layer.average_forward(dataset= A1 )\n",
        "  Z2 = Pool_Layer.pool_out\n",
        "\n",
        "  A2 = Z2.reshape(1797, -1)\n",
        "\n",
        "\n",
        "  Z3 = np.dot(Weights, A2.T)\n",
        "  A3 = softmax(Z3, axis = 0)\n",
        "\n",
        "  loss = CCE(Y,A3.T ).numpy()\n",
        "\n",
        "  My_Losses.append(loss)\n",
        "\n",
        "\n",
        "  # Backward\n",
        "\n",
        "  if i %20 == 0:\n",
        "    print(\"Loss at epoch: {} is: {}\".format(i, loss))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ## BACKWARD PROPOGATION\n",
        "  DL_Z3 = A3.T - Y\n",
        "  DL_Weights = DL_Z3.T.dot(A2)\n",
        "  \n",
        "  DL_A2 = DL_Z3.dot(Weights)\n",
        "  DL_Z2 = DL_A2.reshape(Z2.shape)\n",
        "  \n",
        "\n",
        "  Pool_Layer.average_backward(DL_nextL= DL_Z2)\n",
        "  DL_A1 = Pool_Layer.DL_prev_Layer\n",
        "\n",
        "\n",
        "  DL_Z1 = DL_A1 * relu_derivative(Z1)\n",
        "\n",
        "  Conv_Layer.backward_prop(DL_nextL= DL_Z1, learning_rate = learning_rate)\n",
        "  Weights -= learning_rate * (DL_Weights / num_samples)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epoch: 0 is: 2.3192756175994873\n",
            "Loss at epoch: 20 is: 2.2082409858703613\n",
            "Loss at epoch: 40 is: 1.987061619758606\n",
            "Loss at epoch: 60 is: 1.5277098417282104\n",
            "Loss at epoch: 80 is: 1.0526002645492554\n",
            "Loss at epoch: 100 is: 0.7638491988182068\n",
            "Loss at epoch: 120 is: 0.6031661033630371\n",
            "Loss at epoch: 140 is: 0.5073283910751343\n",
            "Loss at epoch: 160 is: 0.4442830979824066\n",
            "Loss at epoch: 180 is: 0.39931055903434753\n",
            "Loss at epoch: 200 is: 0.3651742935180664\n",
            "Loss at epoch: 220 is: 0.33817368745803833\n",
            "Loss at epoch: 240 is: 0.3160955011844635\n",
            "Loss at epoch: 260 is: 0.29763540625572205\n",
            "Loss at epoch: 280 is: 0.2819855511188507\n",
            "Loss at epoch: 300 is: 0.268539160490036\n",
            "Loss at epoch: 320 is: 0.2568622827529907\n",
            "Loss at epoch: 340 is: 0.24664486944675446\n",
            "Loss at epoch: 360 is: 0.23762474954128265\n",
            "Loss at epoch: 380 is: 0.22960610687732697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a4Z9b193XXa",
        "outputId": "626d32df-63cc-4e36-a102-23a6cc02af19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DL_Z3.shape, A2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1797, 10), (1797, 27))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0bbvbKQh0CC",
        "outputId": "db909912-e042-4df4-e5f7-602a9926f30a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "keras_model = keras.Sequential()\n",
        "\n",
        "keras_model.add(keras.layers.Conv2D(filters= num_filters, kernel_size= (fil_h,fil_w),strides = (1,1), activation= \"relu\",\n",
        "                                    input_shape = (8,8,1), padding  = 'valid',use_bias = False  ))\n",
        "\n",
        "keras_model.add(keras.layers.AveragePooling2D(pool_size=(,),strides=(1,1), padding = 'valid'))\n",
        "\n",
        "keras_model.add(keras.layers.Flatten())\n",
        "\n",
        "keras_model.add(keras.layers.Dense(num_outs,\n",
        "                                   activation= \"softmax\",use_bias=False))\n",
        "\n",
        "keras_model.summary() \n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-2, nesterov=False)\n",
        "\n",
        "keras_model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 4, 4, 3)           75        \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 3, 3, 3)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 27)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                270       \n",
            "=================================================================\n",
            "Total params: 345\n",
            "Trainable params: 345\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKTazo9x0baz",
        "outputId": "4d9a6b7f-7614-4c1d-9066-f59fca213485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "keras_model.fit(x=X,y=Y,epochs=run_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 2.2628 - accuracy: 0.1937\n",
            "Epoch 2/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 2.0495 - accuracy: 0.2421\n",
            "Epoch 3/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 1.8123 - accuracy: 0.3400\n",
            "Epoch 4/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 1.2986 - accuracy: 0.5587\n",
            "Epoch 5/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.9168 - accuracy: 0.7095\n",
            "Epoch 6/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.6970 - accuracy: 0.7819\n",
            "Epoch 7/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.8258\n",
            "Epoch 8/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.8453\n",
            "Epoch 9/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8726\n",
            "Epoch 10/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.3767 - accuracy: 0.8843\n",
            "Epoch 11/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.3406 - accuracy: 0.8937\n",
            "Epoch 12/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.9026\n",
            "Epoch 13/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.8926\n",
            "Epoch 14/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.9110\n",
            "Epoch 15/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.9149\n",
            "Epoch 16/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2524 - accuracy: 0.9226\n",
            "Epoch 17/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2366 - accuracy: 0.9193\n",
            "Epoch 18/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9265\n",
            "Epoch 19/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9277\n",
            "Epoch 20/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2207 - accuracy: 0.9238\n",
            "Epoch 21/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.2070 - accuracy: 0.9388\n",
            "Epoch 22/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1968 - accuracy: 0.9360\n",
            "Epoch 23/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9427\n",
            "Epoch 24/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9432\n",
            "Epoch 25/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1835 - accuracy: 0.9399\n",
            "Epoch 26/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9410\n",
            "Epoch 27/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9460\n",
            "Epoch 28/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1826 - accuracy: 0.9388\n",
            "Epoch 29/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9488\n",
            "Epoch 30/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9471\n",
            "Epoch 31/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1548 - accuracy: 0.9516\n",
            "Epoch 32/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9477\n",
            "Epoch 33/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9527\n",
            "Epoch 34/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9499\n",
            "Epoch 35/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9538\n",
            "Epoch 36/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9521\n",
            "Epoch 37/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1384 - accuracy: 0.9533\n",
            "Epoch 38/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9605\n",
            "Epoch 39/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9605\n",
            "Epoch 40/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9622\n",
            "Epoch 41/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9605\n",
            "Epoch 42/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9655\n",
            "Epoch 43/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9616\n",
            "Epoch 44/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9638\n",
            "Epoch 45/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9616\n",
            "Epoch 46/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9655\n",
            "Epoch 47/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9649\n",
            "Epoch 48/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9594\n",
            "Epoch 49/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1097 - accuracy: 0.9666\n",
            "Epoch 50/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1200 - accuracy: 0.9599\n",
            "Epoch 51/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9672\n",
            "Epoch 52/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 0.9705\n",
            "Epoch 53/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9655\n",
            "Epoch 54/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9661\n",
            "Epoch 55/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9711\n",
            "Epoch 56/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9744\n",
            "Epoch 57/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9716\n",
            "Epoch 58/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9755\n",
            "Epoch 59/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9699\n",
            "Epoch 60/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9755\n",
            "Epoch 61/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9738\n",
            "Epoch 62/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9755\n",
            "Epoch 63/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0934 - accuracy: 0.9738\n",
            "Epoch 64/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0870 - accuracy: 0.9705\n",
            "Epoch 65/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0826 - accuracy: 0.9744\n",
            "Epoch 66/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9755\n",
            "Epoch 67/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9755\n",
            "Epoch 68/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9733\n",
            "Epoch 69/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9761\n",
            "Epoch 70/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9750\n",
            "Epoch 71/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9800\n",
            "Epoch 72/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9811\n",
            "Epoch 73/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0735 - accuracy: 0.9783\n",
            "Epoch 74/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9800\n",
            "Epoch 75/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0702 - accuracy: 0.9816\n",
            "Epoch 76/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.9794\n",
            "Epoch 77/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0689 - accuracy: 0.9827\n",
            "Epoch 78/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 0.9777\n",
            "Epoch 79/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9805\n",
            "Epoch 80/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9800\n",
            "Epoch 81/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.9816\n",
            "Epoch 82/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9805\n",
            "Epoch 83/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0678 - accuracy: 0.9811\n",
            "Epoch 84/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9833\n",
            "Epoch 85/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9811\n",
            "Epoch 86/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9822\n",
            "Epoch 87/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9822\n",
            "Epoch 88/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9844\n",
            "Epoch 89/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9805\n",
            "Epoch 90/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9822\n",
            "Epoch 91/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9844\n",
            "Epoch 92/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.9866\n",
            "Epoch 93/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9850\n",
            "Epoch 94/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.9850\n",
            "Epoch 95/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9839\n",
            "Epoch 96/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.9827\n",
            "Epoch 97/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9855\n",
            "Epoch 98/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9827\n",
            "Epoch 99/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.9816\n",
            "Epoch 100/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0586 - accuracy: 0.9866\n",
            "Epoch 101/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0536 - accuracy: 0.9866\n",
            "Epoch 102/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9839\n",
            "Epoch 103/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9844\n",
            "Epoch 104/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9844\n",
            "Epoch 105/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0521 - accuracy: 0.9866\n",
            "Epoch 106/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0541 - accuracy: 0.9839\n",
            "Epoch 107/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0541 - accuracy: 0.9866\n",
            "Epoch 108/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0520 - accuracy: 0.9850\n",
            "Epoch 109/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9861\n",
            "Epoch 110/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9894\n",
            "Epoch 111/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9839\n",
            "Epoch 112/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9866\n",
            "Epoch 113/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9866\n",
            "Epoch 114/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0493 - accuracy: 0.9861\n",
            "Epoch 115/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9872\n",
            "Epoch 116/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9866\n",
            "Epoch 117/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0456 - accuracy: 0.9866\n",
            "Epoch 118/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9866\n",
            "Epoch 119/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9878\n",
            "Epoch 120/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0472 - accuracy: 0.9866\n",
            "Epoch 121/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9861\n",
            "Epoch 122/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0456 - accuracy: 0.9866\n",
            "Epoch 123/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0456 - accuracy: 0.9861\n",
            "Epoch 124/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0447 - accuracy: 0.9878\n",
            "Epoch 125/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9866\n",
            "Epoch 126/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0467 - accuracy: 0.9861\n",
            "Epoch 127/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9889\n",
            "Epoch 128/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0505 - accuracy: 0.9844\n",
            "Epoch 129/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9889\n",
            "Epoch 130/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9900\n",
            "Epoch 131/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 0.9911\n",
            "Epoch 132/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9844\n",
            "Epoch 133/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9878\n",
            "Epoch 134/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9872\n",
            "Epoch 135/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9911\n",
            "Epoch 136/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9889\n",
            "Epoch 137/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.9900\n",
            "Epoch 138/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9850\n",
            "Epoch 139/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9889\n",
            "Epoch 140/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 0.9911\n",
            "Epoch 141/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9889\n",
            "Epoch 142/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0386 - accuracy: 0.9900\n",
            "Epoch 143/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9872\n",
            "Epoch 144/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0395 - accuracy: 0.9872\n",
            "Epoch 145/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0403 - accuracy: 0.9872\n",
            "Epoch 146/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0372 - accuracy: 0.9900\n",
            "Epoch 147/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.9911\n",
            "Epoch 148/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0360 - accuracy: 0.9911\n",
            "Epoch 149/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9911\n",
            "Epoch 150/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9894\n",
            "Epoch 151/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.9894\n",
            "Epoch 152/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9900\n",
            "Epoch 153/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9900\n",
            "Epoch 154/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9878\n",
            "Epoch 155/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9861\n",
            "Epoch 156/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 0.9917\n",
            "Epoch 157/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 0.9900\n",
            "Epoch 158/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9917\n",
            "Epoch 159/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9900\n",
            "Epoch 160/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0352 - accuracy: 0.9905\n",
            "Epoch 161/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9905\n",
            "Epoch 162/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9928\n",
            "Epoch 163/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0321 - accuracy: 0.9917\n",
            "Epoch 164/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.9911\n",
            "Epoch 165/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.9928\n",
            "Epoch 166/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.9922\n",
            "Epoch 167/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9917\n",
            "Epoch 168/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9928\n",
            "Epoch 169/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0319 - accuracy: 0.9928\n",
            "Epoch 170/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0321 - accuracy: 0.9917\n",
            "Epoch 171/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.9917\n",
            "Epoch 172/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.9928\n",
            "Epoch 173/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.9905\n",
            "Epoch 174/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.9939\n",
            "Epoch 175/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 0.9894\n",
            "Epoch 176/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0312 - accuracy: 0.9928\n",
            "Epoch 177/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0308 - accuracy: 0.9928\n",
            "Epoch 178/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9917\n",
            "Epoch 179/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9922\n",
            "Epoch 180/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9917\n",
            "Epoch 181/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9922\n",
            "Epoch 182/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9944\n",
            "Epoch 183/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9933\n",
            "Epoch 184/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9933\n",
            "Epoch 185/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9944\n",
            "Epoch 186/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.9911\n",
            "Epoch 187/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.9911\n",
            "Epoch 188/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9911\n",
            "Epoch 189/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9944\n",
            "Epoch 190/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9928\n",
            "Epoch 191/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9889\n",
            "Epoch 192/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0284 - accuracy: 0.9928\n",
            "Epoch 193/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9917\n",
            "Epoch 194/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.9939\n",
            "Epoch 195/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.9928\n",
            "Epoch 196/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.9928\n",
            "Epoch 197/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 0.9922\n",
            "Epoch 198/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0265 - accuracy: 0.9944\n",
            "Epoch 199/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9939\n",
            "Epoch 200/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9939\n",
            "Epoch 201/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0269 - accuracy: 0.9922\n",
            "Epoch 202/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.9939\n",
            "Epoch 203/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.9928\n",
            "Epoch 204/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.9933\n",
            "Epoch 205/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9944\n",
            "Epoch 206/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9928\n",
            "Epoch 207/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9944\n",
            "Epoch 208/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.9950\n",
            "Epoch 209/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9950\n",
            "Epoch 210/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 0.9911\n",
            "Epoch 211/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.9928\n",
            "Epoch 212/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.9950\n",
            "Epoch 213/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9933\n",
            "Epoch 214/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.9967\n",
            "Epoch 215/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9933\n",
            "Epoch 216/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9922\n",
            "Epoch 217/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.9967\n",
            "Epoch 218/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.9933\n",
            "Epoch 219/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9933\n",
            "Epoch 220/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9905\n",
            "Epoch 221/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.9944\n",
            "Epoch 222/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9878\n",
            "Epoch 223/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9944\n",
            "Epoch 224/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9939\n",
            "Epoch 225/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.9961\n",
            "Epoch 226/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9933\n",
            "Epoch 227/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9955\n",
            "Epoch 228/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.9961\n",
            "Epoch 229/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.9933\n",
            "Epoch 230/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9950\n",
            "Epoch 231/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 0.9972\n",
            "Epoch 232/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9933\n",
            "Epoch 233/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.9950\n",
            "Epoch 234/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9944\n",
            "Epoch 235/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9950\n",
            "Epoch 236/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.9950\n",
            "Epoch 237/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9939\n",
            "Epoch 238/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.9944\n",
            "Epoch 239/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.9939\n",
            "Epoch 240/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.9944\n",
            "Epoch 241/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.9950\n",
            "Epoch 242/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.9967\n",
            "Epoch 243/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9972\n",
            "Epoch 244/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.9967\n",
            "Epoch 245/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 0.9950\n",
            "Epoch 246/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9972\n",
            "Epoch 247/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 0.9950\n",
            "Epoch 248/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0182 - accuracy: 0.9961\n",
            "Epoch 249/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.9950\n",
            "Epoch 250/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9967\n",
            "Epoch 251/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9967\n",
            "Epoch 252/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9967\n",
            "Epoch 253/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9961\n",
            "Epoch 254/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9967\n",
            "Epoch 255/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.9955\n",
            "Epoch 256/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.9944\n",
            "Epoch 257/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.9955\n",
            "Epoch 258/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9967\n",
            "Epoch 259/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9961\n",
            "Epoch 260/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9961\n",
            "Epoch 261/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9961\n",
            "Epoch 262/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9972\n",
            "Epoch 263/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9955\n",
            "Epoch 264/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9961\n",
            "Epoch 265/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 0.9967\n",
            "Epoch 266/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9972\n",
            "Epoch 267/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9950\n",
            "Epoch 268/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9972\n",
            "Epoch 269/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9955\n",
            "Epoch 270/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9944\n",
            "Epoch 271/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9978\n",
            "Epoch 272/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9967\n",
            "Epoch 273/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.9972\n",
            "Epoch 274/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9961\n",
            "Epoch 275/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.9972\n",
            "Epoch 276/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.9978\n",
            "Epoch 277/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.9967\n",
            "Epoch 278/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 0.9972\n",
            "Epoch 279/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9967\n",
            "Epoch 280/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9978\n",
            "Epoch 281/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9978\n",
            "Epoch 282/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9961\n",
            "Epoch 283/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9967\n",
            "Epoch 284/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9978\n",
            "Epoch 285/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9961\n",
            "Epoch 286/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9989\n",
            "Epoch 287/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9967\n",
            "Epoch 288/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9978\n",
            "Epoch 289/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9978\n",
            "Epoch 290/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9955\n",
            "Epoch 291/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9961\n",
            "Epoch 292/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9950\n",
            "Epoch 293/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9972\n",
            "Epoch 294/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9978\n",
            "Epoch 295/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9978\n",
            "Epoch 296/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9978\n",
            "Epoch 297/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9978\n",
            "Epoch 298/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9978\n",
            "Epoch 299/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9983\n",
            "Epoch 300/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9983\n",
            "Epoch 301/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9983\n",
            "Epoch 302/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9967\n",
            "Epoch 303/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9978\n",
            "Epoch 304/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9967\n",
            "Epoch 305/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9978\n",
            "Epoch 306/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0134 - accuracy: 0.9978\n",
            "Epoch 307/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.9983\n",
            "Epoch 308/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9972\n",
            "Epoch 309/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9978\n",
            "Epoch 310/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9978\n",
            "Epoch 311/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9983\n",
            "Epoch 312/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9983\n",
            "Epoch 313/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9955\n",
            "Epoch 314/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 0.9978\n",
            "Epoch 315/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9967\n",
            "Epoch 316/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9989\n",
            "Epoch 317/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9983\n",
            "Epoch 318/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9983\n",
            "Epoch 319/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9972\n",
            "Epoch 320/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9983\n",
            "Epoch 321/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9978\n",
            "Epoch 322/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9972\n",
            "Epoch 323/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9989\n",
            "Epoch 324/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9983\n",
            "Epoch 325/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9989\n",
            "Epoch 326/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9983\n",
            "Epoch 327/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9989\n",
            "Epoch 328/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9994\n",
            "Epoch 329/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9994\n",
            "Epoch 330/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 331/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9978\n",
            "Epoch 332/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9994\n",
            "Epoch 333/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9989\n",
            "Epoch 334/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9989\n",
            "Epoch 335/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9983\n",
            "Epoch 336/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9972\n",
            "Epoch 337/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9989\n",
            "Epoch 338/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9989\n",
            "Epoch 339/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9978\n",
            "Epoch 340/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9983\n",
            "Epoch 341/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9994\n",
            "Epoch 342/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9978\n",
            "Epoch 343/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9989\n",
            "Epoch 344/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0111 - accuracy: 0.9994\n",
            "Epoch 345/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9983\n",
            "Epoch 346/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9989\n",
            "Epoch 347/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9994\n",
            "Epoch 348/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9989\n",
            "Epoch 349/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9983\n",
            "Epoch 350/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9994\n",
            "Epoch 351/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9989\n",
            "Epoch 352/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9989\n",
            "Epoch 353/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9983\n",
            "Epoch 354/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9983\n",
            "Epoch 355/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9983\n",
            "Epoch 356/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9994\n",
            "Epoch 357/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9983\n",
            "Epoch 358/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9994\n",
            "Epoch 359/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9989\n",
            "Epoch 360/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9994\n",
            "Epoch 361/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9983\n",
            "Epoch 362/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9989\n",
            "Epoch 363/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9994\n",
            "Epoch 364/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9994\n",
            "Epoch 365/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9994\n",
            "Epoch 366/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9994\n",
            "Epoch 367/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 368/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9994\n",
            "Epoch 369/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9994\n",
            "Epoch 370/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9994\n",
            "Epoch 371/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9989\n",
            "Epoch 372/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9989\n",
            "Epoch 373/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 374/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 375/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9994\n",
            "Epoch 376/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9989\n",
            "Epoch 377/400\n",
            "57/57 [==============================] - 0s 1ms/step - loss: 0.0090 - accuracy: 0.9994\n",
            "Epoch 378/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9989\n",
            "Epoch 379/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 380/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9994\n",
            "Epoch 381/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9994\n",
            "Epoch 382/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 383/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9989\n",
            "Epoch 384/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9994\n",
            "Epoch 385/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9994\n",
            "Epoch 386/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9994\n",
            "Epoch 387/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9989\n",
            "Epoch 388/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9983\n",
            "Epoch 389/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 390/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9994\n",
            "Epoch 391/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9994\n",
            "Epoch 392/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9994\n",
            "Epoch 393/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9994\n",
            "Epoch 394/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9994\n",
            "Epoch 395/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9989\n",
            "Epoch 396/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 397/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 398/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 399/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9994\n",
            "Epoch 400/400\n",
            "57/57 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe17f41ce48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7HJ7jYP0lY1",
        "outputId": "051149e4-28da-4c98-c406-431fed60909a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "A3[:, 0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.54136155e-04, 1.30641107e-02, 3.55575214e-04, 2.38883405e-02,\n",
              "       8.59348677e-07, 4.16441191e-04, 1.65036762e-07, 1.15031086e-05,\n",
              "       6.19909768e-03, 9.55809771e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOJwaH4e2jhw",
        "outputId": "aa2d86f6-9184-4652-bd80-7d0a0ec3da99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y[0, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLomTMkM2pfW",
        "outputId": "8b085b75-88db-46c8-e00f-91588c177037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
        "plt.plot(My_Losses, color = 'y')\n",
        "plt.plot(keras_model.history.history[\"loss\"], color = 'g')\n",
        "plt.legend([\"Own Implementation\", \"Keras Implementation\"])\n",
        "plt.title(\"Loss of algorithm at each epoch, comparing own and keras implementation: \\n\")\n",
        "plt.xlabel(\"Num of epochs \")\n",
        "plt.ylabel(\"Loss at each epoch\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihmSShGfBs38"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}