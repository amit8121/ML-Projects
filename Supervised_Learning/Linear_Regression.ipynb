{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.748084Z",
     "start_time": "2020-05-17T02:22:48.859304Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy import linalg\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_boston, fetch_california_housing\n",
    "\n",
    "from sklearn.linear_model  import LinearRegression, SGDRegressor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.763935Z",
     "start_time": "2020-05-17T02:22:49.748968Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = load_boston()\n",
    "dataset = fetch_california_housing()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "\n",
    "normalize = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "\n",
    "X_train = X[:450, :]\n",
    "# X_train = normalize(X_train)\n",
    "y_train = y[:450].reshape(-1,1)\n",
    "\n",
    "X_test = X[450:, :]\n",
    "y_test = y[450:].reshape(-1,1)\n",
    "\n",
    "num_rows = X_train.shape[0]\n",
    "num_cols = X_train.shape[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.770904Z",
     "start_time": "2020-05-17T02:22:49.765917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((450, 8), (450, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.776872Z",
     "start_time": "2020-05-17T02:22:49.772902Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_rows = int(input(\"number of rows:\"))\n",
    "# num_cols = int(input(\"number of columns: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.782879Z",
     "start_time": "2020-05-17T02:22:49.777853Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train = 2 * np.random.rand(num_rows,num_cols)\n",
    "# y_train = 4 + 3 * X_train[:, 0].reshape(-1,1) + np.random.randn(num_rows,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.888558Z",
     "start_time": "2020-05-17T02:22:49.783871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Random plot of X and Y\")\n",
    "plt.scatter(X_train[:, 0], y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.895539Z",
     "start_time": "2020-05-17T02:22:49.890552Z"
    }
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((num_rows,1)), X_train]\n",
    "\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)\n",
    "\n",
    "# theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.904516Z",
     "start_time": "2020-05-17T02:22:49.896537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.00000000e+00,  8.30140000e+00,  2.10000000e+01,  6.23813708e+00,\n",
       "         9.71880492e-01,  2.40100000e+03,  2.10984183e+00,  3.78600000e+01,\n",
       "        -1.22220000e+02]), array([3.585]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b[1], y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.911538Z",
     "start_time": "2020-05-17T02:22:49.905512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.18096740e+01],\n",
       "       [ 3.01306608e-01],\n",
       "       [-5.35048731e-03],\n",
       "       [ 2.81635884e-02],\n",
       "       [-7.80011633e-01],\n",
       "       [ 1.28119394e-04],\n",
       "       [-7.61519429e-02],\n",
       "       [ 9.02959708e+00],\n",
       "       [ 2.02675729e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.918480Z",
     "start_time": "2020-05-17T02:22:49.912493Z"
    }
   },
   "outputs": [],
   "source": [
    "theta = np.random.randn(num_cols+1,1) * 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.924498Z",
     "start_time": "2020-05-17T02:22:49.920473Z"
    }
   },
   "outputs": [],
   "source": [
    "# theta = np.random.uniform(-1/num_cols+1 , 1/num_cols+1, (num_cols+1,1) ) *1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.932469Z",
     "start_time": "2020-05-17T02:22:49.925490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00856178],\n",
       "       [ 0.0042269 ],\n",
       "       [-0.00993854],\n",
       "       [ 0.01139976],\n",
       "       [-0.00766401],\n",
       "       [ 0.01326484],\n",
       "       [-0.00924206],\n",
       "       [ 0.01189839],\n",
       "       [-0.0176063 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:49.938427Z",
     "start_time": "2020-05-17T02:22:49.933438Z"
    }
   },
   "outputs": [],
   "source": [
    "# mse = lambda y, yhat: np.mean([(y-yhat)**2 for i in range(len(y))])\n",
    "\n",
    "mse = lambda y, yhat: mean_squared_error(y_true = y, y_pred = yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.386227Z",
     "start_time": "2020-05-17T02:22:49.939424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 217.84028380445173 at iteration : 0 \t\n",
      "R2 Score: -242.4317410815481 at iteration: 0 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 2 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 2 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 4 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 4 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 6 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 6 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 8 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 8 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 10 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 10 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 12 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 12 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 14 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 14 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 16 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 16 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 18 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 18 \n",
      "\n",
      "Loss: 126.65856207505156 at iteration : 20 \t\n",
      "R2 Score: -140.53816617541997 at iteration: 20 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 22 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 22 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 24 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 24 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 26 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 26 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 28 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 28 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 30 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 30 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 32 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 32 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 34 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 34 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 36 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 36 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 38 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 38 \n",
      "\n",
      "Loss: 73.79855524203361 at iteration : 40 \t\n",
      "R2 Score: -81.46826747617341 at iteration: 40 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 42 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 42 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 44 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 44 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 46 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 46 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 48 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 48 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 50 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 50 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 52 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 52 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 54 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 54 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 56 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 56 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 58 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 58 \n",
      "\n",
      "Loss: 43.154467840663365 at iteration : 60 \t\n",
      "R2 Score: -47.22417166574439 at iteration: 60 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 62 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 62 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 64 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 64 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 66 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 66 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 68 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 68 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 70 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 70 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 72 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 72 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 74 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 74 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 76 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 76 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 78 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 78 \n",
      "\n",
      "Loss: 25.3894250690069 at iteration : 80 \t\n",
      "R2 Score: -27.37212586059599 at iteration: 80 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 82 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 82 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 84 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 84 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 86 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 86 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 88 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 88 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 90 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 90 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 92 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 92 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 94 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 94 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 96 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 96 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 98 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 98 \n",
      "\n",
      "Loss: 15.090640501329137 at iteration : 100 \t\n",
      "R2 Score: -15.863459903366163 at iteration: 100 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 102 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 102 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 104 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 104 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 106 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 106 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 108 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 108 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 110 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 110 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 112 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 112 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 114 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 114 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 116 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 116 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 118 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 118 \n",
      "\n",
      "Loss: 9.120205842590448 at iteration : 120 \t\n",
      "R2 Score: -9.191630071859691 at iteration: 120 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 122 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 122 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 124 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 124 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 126 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 126 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 128 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 128 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 130 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 130 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 132 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 132 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 134 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 134 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 136 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 136 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 138 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 138 \n",
      "\n",
      "Loss: 5.659008404115719 at iteration : 140 \t\n",
      "R2 Score: -5.323817819874004 at iteration: 140 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 142 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 142 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 144 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 144 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 146 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 146 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 148 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 148 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 150 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 150 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 152 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 152 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 154 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 154 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 156 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 156 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 158 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 158 \n",
      "\n",
      "Loss: 3.652469573978167 at iteration : 160 \t\n",
      "R2 Score: -3.0815546698379555 at iteration: 160 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 162 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 162 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 164 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 164 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 166 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 166 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 168 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 168 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 170 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 170 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 172 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 172 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 174 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 174 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 176 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 176 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 178 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 178 \n",
      "\n",
      "Loss: 2.4892275296372017 at iteration : 180 \t\n",
      "R2 Score: -1.7816571889506587 at iteration: 180 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 182 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 182 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 184 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 184 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 186 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 186 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 188 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 188 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 190 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 190 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 192 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 192 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 194 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 194 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 196 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 196 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 198 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 198 \n",
      "\n",
      "Loss: 1.814862720150262 at iteration : 200 \t\n",
      "R2 Score: -1.0280692995550726 at iteration: 200 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 202 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 202 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 204 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 204 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 206 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 206 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 208 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 208 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 210 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 210 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 212 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 212 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 214 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 214 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 216 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 216 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 218 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 218 \n",
      "\n",
      "Loss: 1.4239105360728097 at iteration : 220 \t\n",
      "R2 Score: -0.5911888053346404 at iteration: 220 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 222 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 222 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 224 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 224 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 226 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 226 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 228 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 228 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 230 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 230 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 232 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 232 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 234 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 234 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 236 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 236 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 238 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 238 \n",
      "\n",
      "Loss: 1.197258739871294 at iteration : 240 \t\n",
      "R2 Score: -0.33791039233861553 at iteration: 240 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 242 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 242 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 244 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 244 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 246 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 246 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 248 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 248 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 250 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 250 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 252 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 252 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 254 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 254 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 256 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 256 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 258 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 258 \n",
      "\n",
      "Loss: 1.0658554146679615 at iteration : 260 \t\n",
      "R2 Score: -0.19107005739456806 at iteration: 260 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 262 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 262 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 264 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 264 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 266 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 266 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 268 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 268 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 270 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 270 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 272 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 272 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 274 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 274 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 276 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 276 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 278 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 278 \n",
      "\n",
      "Loss: 0.9896696843359307 at iteration : 280 \t\n",
      "R2 Score: -0.10593417409327865 at iteration: 280 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 282 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 282 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 284 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 284 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 286 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 286 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 288 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 288 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 290 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 290 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 292 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 292 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 294 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 294 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 296 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 296 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 298 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 298 \n",
      "\n",
      "Loss: 0.9454947920354451 at iteration : 300 \t\n",
      "R2 Score: -0.05656970046612231 at iteration: 300 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 302 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 302 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 304 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 304 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 306 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 306 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 308 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 308 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 310 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 310 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 312 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 312 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 314 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 314 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 316 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 316 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 318 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 318 \n",
      "\n",
      "Loss: 0.9198772834432869 at iteration : 320 \t\n",
      "R2 Score: -0.0279426962690541 at iteration: 320 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 322 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 322 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 324 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 324 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 326 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 326 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 328 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 328 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 330 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 330 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 332 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 332 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 334 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 334 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 336 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 336 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 338 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 338 \n",
      "\n",
      "Loss: 0.9050179027328161 at iteration : 340 \t\n",
      "R2 Score: -0.011337664111684242 at iteration: 340 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 342 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 342 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 344 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 344 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 346 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 346 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 348 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 348 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 350 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 350 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 352 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 352 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 354 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 354 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 356 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 356 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 358 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 358 \n",
      "\n",
      "Loss: 0.8963952541237417 at iteration : 360 \t\n",
      "R2 Score: -0.0017020433395151802 at iteration: 360 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 362 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 362 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 364 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 364 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 366 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 366 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 368 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 368 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 370 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 370 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 372 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 372 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 374 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 374 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 376 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 376 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 378 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 378 \n",
      "\n",
      "Loss: 0.8913881875540787 at iteration : 380 \t\n",
      "R2 Score: 0.003893243773948285 at iteration: 380 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 382 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 382 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 384 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 384 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 386 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 386 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 388 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 388 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 390 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 390 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 392 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 392 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 394 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 394 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 396 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 396 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 398 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 398 \n",
      "\n",
      "Loss: 0.8884771661298709 at iteration : 400 \t\n",
      "R2 Score: 0.0071462463924020225 at iteration: 400 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 402 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 402 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 404 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 404 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 406 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 406 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 408 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 408 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 410 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 410 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 412 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 412 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 414 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 414 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 416 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 416 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 418 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 418 \n",
      "\n",
      "Loss: 0.8867812817199197 at iteration : 420 \t\n",
      "R2 Score: 0.009041360038866353 at iteration: 420 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 422 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 422 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 424 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 424 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 426 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 426 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 428 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 428 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 430 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 430 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 432 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 432 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 434 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 434 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 436 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 436 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 438 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 438 \n",
      "\n",
      "Loss: 0.8857898529736071 at iteration : 440 \t\n",
      "R2 Score: 0.010149259926151788 at iteration: 440 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 442 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 442 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 444 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 444 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 446 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 446 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 448 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 448 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 450 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 450 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 452 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 452 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 454 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 454 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 456 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 456 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 458 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 458 \n",
      "\n",
      "Loss: 0.8852068269395783 at iteration : 460 \t\n",
      "R2 Score: 0.010800778736542727 at iteration: 460 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 462 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 462 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 464 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 464 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 466 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 466 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 468 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 468 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 470 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 470 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 472 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 472 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 474 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 474 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 476 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 476 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 478 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 478 \n",
      "\n",
      "Loss: 0.8848605753493863 at iteration : 480 \t\n",
      "R2 Score: 0.011187707297140581 at iteration: 480 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.8846516015899241 at iteration : 482 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 482 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 484 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 484 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 486 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 486 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 488 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 488 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 490 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 490 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 492 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 492 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 494 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 494 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 496 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 496 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 498 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 498 \n",
      "\n",
      "Loss: 0.8846516015899241 at iteration : 500 \t\n",
      "R2 Score: 0.011421230891665135 at iteration: 500 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 502 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 502 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 504 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 504 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 506 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 506 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 508 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 508 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 510 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 510 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 512 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 512 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 514 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 514 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 516 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 516 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 518 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 518 \n",
      "\n",
      "Loss: 0.8845222253146247 at iteration : 520 \t\n",
      "R2 Score: 0.0115658060427839 at iteration: 520 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 522 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 522 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 524 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 524 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 526 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 526 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 528 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 528 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 530 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 530 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 532 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 532 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 534 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 534 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 536 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 536 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 538 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 538 \n",
      "\n",
      "Loss: 0.8844390079628708 at iteration : 540 \t\n",
      "R2 Score: 0.011658799608858295 at iteration: 540 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 542 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 542 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 544 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 544 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 546 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 546 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 548 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 548 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 550 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 550 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 552 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 552 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 554 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 554 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 556 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 556 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 558 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 558 \n",
      "\n",
      "Loss: 0.8843825644569502 at iteration : 560 \t\n",
      "R2 Score: 0.011721873989220866 at iteration: 560 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 562 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 562 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 564 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 564 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 566 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 566 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 568 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 568 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 570 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 570 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 572 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 572 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 574 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 574 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 576 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 576 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 578 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 578 \n",
      "\n",
      "Loss: 0.8843416568244288 at iteration : 580 \t\n",
      "R2 Score: 0.01176758737167849 at iteration: 580 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 582 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 582 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 584 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 584 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 586 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 586 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 588 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 588 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 590 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 590 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 592 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 592 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 594 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 594 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 596 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 596 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 598 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 598 \n",
      "\n",
      "Loss: 0.8843097701444592 at iteration : 600 \t\n",
      "R2 Score: 0.011803220037440498 at iteration: 600 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 602 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 602 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 604 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 604 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 606 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 606 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 608 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 608 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 610 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 610 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 612 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 612 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 614 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 614 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 616 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 616 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 618 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 618 \n",
      "\n",
      "Loss: 0.8842831275504717 at iteration : 620 \t\n",
      "R2 Score: 0.011832992552092247 at iteration: 620 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 622 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 622 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 624 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 624 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 626 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 626 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 628 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 628 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 630 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 630 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 632 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 632 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 634 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 634 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 636 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 636 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 638 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 638 \n",
      "\n",
      "Loss: 0.8842595394846944 at iteration : 640 \t\n",
      "R2 Score: 0.011859351698438436 at iteration: 640 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 642 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 642 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 644 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 644 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 646 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 646 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 648 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 648 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 650 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 650 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 652 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 652 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 654 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 654 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 656 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 656 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 658 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 658 \n",
      "\n",
      "Loss: 0.8842377365863349 at iteration : 660 \t\n",
      "R2 Score: 0.011883715959334573 at iteration: 660 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 662 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 662 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 664 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 664 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 666 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 666 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 668 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 668 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 670 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 670 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 672 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 672 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 674 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 674 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 676 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 676 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 678 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 678 \n",
      "\n",
      "Loss: 0.8842169829525625 at iteration : 680 \t\n",
      "R2 Score: 0.01190690769005931 at iteration: 680 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 682 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 682 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 684 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 684 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 686 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 686 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 688 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 688 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 690 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 690 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 692 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 692 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 694 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 694 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 696 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 696 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 698 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 698 \n",
      "\n",
      "Loss: 0.8841968519376108 at iteration : 700 \t\n",
      "R2 Score: 0.011929403657902626 at iteration: 700 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 702 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 702 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 704 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 704 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 706 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 706 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 708 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 708 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 710 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 710 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 712 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 712 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 714 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 714 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 716 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 716 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 718 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 718 \n",
      "\n",
      "Loss: 0.8841770961787134 at iteration : 720 \t\n",
      "R2 Score: 0.011951480285332328 at iteration: 720 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 722 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 722 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 724 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 724 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 726 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 726 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 728 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 728 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 730 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 730 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 732 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 732 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 734 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 734 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 736 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 736 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 738 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 738 \n",
      "\n",
      "Loss: 0.8841575722473516 at iteration : 740 \t\n",
      "R2 Score: 0.011973297850574105 at iteration: 740 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 742 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 742 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 744 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 744 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 746 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 746 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 748 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 748 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 750 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 750 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 752 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 752 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 754 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 754 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 756 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 756 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 758 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 758 \n",
      "\n",
      "Loss: 0.8841381969679689 at iteration : 760 \t\n",
      "R2 Score: 0.011994949300488167 at iteration: 760 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 762 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 762 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 764 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 764 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 766 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 766 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 768 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 768 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 770 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 770 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 772 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 772 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 774 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 774 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 776 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 776 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 778 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 778 \n",
      "\n",
      "Loss: 0.8841189220949933 at iteration : 780 \t\n",
      "R2 Score: 0.012016488548443727 at iteration: 780 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 782 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 782 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 784 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 784 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 786 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 786 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 788 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 788 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 790 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 790 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 792 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 792 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 794 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 794 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 796 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 796 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 798 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 798 \n",
      "\n",
      "Loss: 0.8840997196325644 at iteration : 800 \t\n",
      "R2 Score: 0.0120379468792009 at iteration: 800 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 802 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 802 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 804 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 804 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 806 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 806 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 808 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 808 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 810 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 810 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 812 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 812 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 814 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 814 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 816 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 816 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 818 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 818 \n",
      "\n",
      "Loss: 0.8840805733240638 at iteration : 820 \t\n",
      "R2 Score: 0.012059342459174505 at iteration: 820 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 822 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 822 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 824 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 824 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 826 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 826 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 828 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 828 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 830 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 830 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 832 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 832 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 834 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 834 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 836 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 836 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 838 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 838 \n",
      "\n",
      "Loss: 0.8840614737184147 at iteration : 840 \t\n",
      "R2 Score: 0.012080685849735318 at iteration: 840 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 842 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 842 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 844 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 844 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 846 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 846 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 848 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 848 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 850 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 850 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 852 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 852 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 854 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 854 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 856 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 856 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 858 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 858 \n",
      "\n",
      "Loss: 0.8840424153099086 at iteration : 860 \t\n",
      "R2 Score: 0.012101983203392175 at iteration: 860 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 862 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 862 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 864 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 864 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 866 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 866 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 868 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 868 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 870 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 870 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 872 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 872 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 874 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 874 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 876 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 876 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 878 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 878 \n",
      "\n",
      "Loss: 0.884023394880102 at iteration : 880 \t\n",
      "R2 Score: 0.01212323811668492 at iteration: 880 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 882 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 882 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 884 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 884 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 886 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 886 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 888 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 888 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 890 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 890 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 892 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 892 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 894 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 894 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.8840044105365786 at iteration : 896 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 896 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 898 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 898 \n",
      "\n",
      "Loss: 0.8840044105365786 at iteration : 900 \t\n",
      "R2 Score: 0.012144452704347386 at iteration: 900 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 902 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 902 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 904 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 904 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 906 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 906 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 908 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 908 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 910 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 910 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 912 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 912 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 914 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 914 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 916 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 916 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 918 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 918 \n",
      "\n",
      "Loss: 0.8839854611556991 at iteration : 920 \t\n",
      "R2 Score: 0.01216562822202194 at iteration: 920 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 922 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 922 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 924 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 924 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 926 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 926 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 928 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 928 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 930 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 930 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 932 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 932 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 934 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 934 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 936 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 936 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 938 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 938 \n",
      "\n",
      "Loss: 0.8839665460595496 at iteration : 940 \t\n",
      "R2 Score: 0.01218676542726227 at iteration: 940 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 942 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 942 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 944 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 944 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 946 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 946 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 948 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 948 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 950 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 950 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 952 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 952 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 954 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 954 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 956 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 956 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 958 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 958 \n",
      "\n",
      "Loss: 0.8839476648286647 at iteration : 960 \t\n",
      "R2 Score: 0.012207864788811973 at iteration: 960 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 962 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 962 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 964 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 964 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 966 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 966 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 968 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 968 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 970 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 970 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 972 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 972 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 974 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 974 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 976 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 976 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 978 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 978 \n",
      "\n",
      "Loss: 0.883928817193455 at iteration : 980 \t\n",
      "R2 Score: 0.012228926607931179 at iteration: 980 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 982 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 982 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 984 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 984 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 986 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 986 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 988 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 988 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 990 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 990 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 992 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 992 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 994 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 994 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 996 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 996 \n",
      "\n",
      "Loss: 0.8839100029712694 at iteration : 998 \t\n",
      "R2 Score: 0.012249951088728506 at iteration: 998 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = X_b.shape[0]\n",
    "eta = 1e-7\n",
    "it = 1000\n",
    "\n",
    "for i in range(it):\n",
    "    \n",
    "    preds = X_b.dot(theta)\n",
    "    \n",
    "    loss = mse(y_train, preds)\n",
    "    r2score = r2_score(y_true = y_train, y_pred = preds)\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        print(\"Loss: {} at iteration : {} \\t\".format(loss, i))\n",
    "        print(\"R2 Score: {} at iteration: {} \\n\".format(r2score, i))\n",
    "    \n",
    "#     ri = np.random.randint(low = 0, high = m, size = 1)\n",
    "    \n",
    "    gradient = 2 / m * ( X_b.T.dot(preds.reshape(-1,1) - y_train)  )\n",
    "    \n",
    "\n",
    "#     for i in range(1, len(gradient)):\n",
    "        \n",
    "#         gradient[i] += (0.45 * np.abs(theta[i]) )\n",
    "        \n",
    "    \n",
    "    if i% 20 == 0:\n",
    "\n",
    "        theta = theta - eta * gradient\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.392210Z",
     "start_time": "2020-05-17T02:22:50.387225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00855057],\n",
       "       [ 0.00420414],\n",
       "       [-0.01041511],\n",
       "       [ 0.01135066],\n",
       "       [-0.00767624],\n",
       "       [-0.00013548],\n",
       "       [-0.00927476],\n",
       "       [ 0.01147477],\n",
       "       [-0.01623603]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.401186Z",
     "start_time": "2020-05-17T02:22:50.393209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.10242694],\n",
       "       [ -2.36930948],\n",
       "       [  2.86682162],\n",
       "       [ -0.69917406],\n",
       "       [  0.16236331],\n",
       "       [ -1.38659174],\n",
       "       [  0.74290447],\n",
       "       [  3.82478929],\n",
       "       [-12.50723506]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.417143Z",
     "start_time": "2020-05-17T02:22:50.413155Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = X_b.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.432105Z",
     "start_time": "2020-05-17T02:22:50.427119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8839100029712694"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.439085Z",
     "start_time": "2020-05-17T02:22:50.434098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012249951088728506"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.563785Z",
     "start_time": "2020-05-17T02:22:50.440082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(theta, bins = len(theta))\n",
    "plt.title(\"WEights without regularization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.566777Z",
     "start_time": "2020-05-17T02:22:50.564749Z"
    }
   },
   "outputs": [],
   "source": [
    "see = X_b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.576722Z",
     "start_time": "2020-05-17T02:22:50.567742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 450)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "see.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.583699Z",
     "start_time": "2020-05-17T02:22:50.577715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2988268888888888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( see[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.590681Z",
     "start_time": "2020-05-17T02:22:50.584696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4086850332547654"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(see[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.597663Z",
     "start_time": "2020-05-17T02:22:50.591677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8462624497223479"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(see[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.604643Z",
     "start_time": "2020-05-17T02:22:50.598659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21338434569976383"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_train, X_b.dot(theta_best) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.614619Z",
     "start_time": "2020-05-17T02:22:50.606638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615476720555993"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, X_b.dot(theta_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.621638Z",
     "start_time": "2020-05-17T02:22:50.616613Z"
    }
   },
   "outputs": [],
   "source": [
    "sgdreg = SGDRegressor(penalty = 'none', max_iter = it, verbose = 5, learning_rate = 'constant', eta0 = eta,\n",
    "                     early_stopping = False, fit_intercept = False, shuffle = False, warm_start = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.639552Z",
     "start_time": "2020-05-17T02:22:50.622595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.00, NNZs: 8, Bias: 0.000000, T: 450, Avg. loss: 0.643165\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.00, NNZs: 8, Bias: 0.000000, T: 900, Avg. loss: 0.486778\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 1350, Avg. loss: 0.399629\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 1800, Avg. loss: 0.342706\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 2250, Avg. loss: 0.305625\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 2700, Avg. loss: 0.281547\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 3150, Avg. loss: 0.265977\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 3600, Avg. loss: 0.255961\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 4050, Avg. loss: 0.249559\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 4500, Avg. loss: 0.245501\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 4950, Avg. loss: 0.242957\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 5400, Avg. loss: 0.241383\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 5850, Avg. loss: 0.240428\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 6300, Avg. loss: 0.239862\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 6750, Avg. loss: 0.239539\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 7200, Avg. loss: 0.239363\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.01, NNZs: 8, Bias: 0.000000, T: 7650, Avg. loss: 0.239274\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 17 epochs took 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=1e-07, fit_intercept=False, l1_ratio=0.15,\n",
       "             learning_rate='constant', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty='none', power_t=0.25,\n",
       "             random_state=None, shuffle=False, tol=0.001,\n",
       "             validation_fraction=0.1, verbose=5, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgdreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.649523Z",
     "start_time": "2020-05-17T02:22:50.641545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.098065506305861"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_train, sgdreg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T02:22:50.656537Z",
     "start_time": "2020-05-17T02:22:50.651518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2270641286052384"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, sgdreg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
